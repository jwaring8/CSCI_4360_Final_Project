{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all our filepaths\n",
    "filepath=r'C:\\Users\\Jonathan\\Desktop\\Data Science 2\\SARC-train.csv'\n",
    "filenameAFINN = r'C:\\Users\\Jonathan\\Desktop\\Data Science 2\\AFINN-111.txt'\n",
    "filenameEmotion = r'C:\\Users\\Jonathan\\Desktop\\Data Science 2\\hashtag-emotion-0.2.txt'\n",
    "\n",
    "#read in data and split into quotes and their responses. We've got some NA responses so axe those entries\n",
    "dataset = pd.read_csv(filepath)\n",
    "dataset=dataset.dropna(axis=0)\n",
    "Y = dataset['Label'].values\n",
    "quotes = dataset['Quote Text'].values\n",
    "responses = dataset['Response Text'].values \n",
    "\n",
    "#read in AFINN and hashtag-emotion databases. Probably shouldve used pandas to do this but whatever\n",
    "afinnMat=[]\n",
    "emotMat=[]\n",
    "for ws in open(filenameAFINN):\n",
    "    afinnMat.append(ws.strip().split('\\t'))\n",
    "for ws in open(filenameEmotion):\n",
    "    emotMat.append(ws.strip().split('\\t'))\n",
    "emotMat=np.array(emotMat)\n",
    "afinnMat=np.array(afinnMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"First off, That's grade A USDA approved Liberalism in a nutshell.\",\n",
       "       \"watch it. Now you're using my lines. Poet has always been an easy target, I will agree. ;)\",\n",
       "       'Because it will encourage teens to engage in riskier behavior. Abstinence until marriage is still the best way.',\n",
       "       ...,\n",
       "       'Sorry, I expanded my definitions here and was not polite enough to inform the rest of you :) I was not meaning to say JUST that nothing would be done about global warming (though I see now that it certainly appears that way). I am saying nothing will be done PERIOD. No preparation, nothing. We will wait until it is too late, act surprised when it happens, and then blame either the mexicans or the chinese. It is what we do.',\n",
       "       'What we are left with are gods that have considerably little interaction with the world that we know.',\n",
       "       '\"They don\\'t want state oversight,\" Wollmer said, \"but they\\'re more than willing to take state dollars.\"'], dtype=object)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, tokenizer, stopwords=stopwords.words(\"english\"), stemming=False, stemmer=PorterStemmer()):\n",
    "    '''\n",
    "    This function will remove stopwords from the text and perform stemming. Return tokenized sentences. \n",
    "    \n",
    "    Params:\n",
    "    text -- string we are looking at \n",
    "    tokenizer -- string of either 'twitter' or 'word' to specify which tokenizer to use\n",
    "    stopwords -- list of stopwords to remove, default is the NLTK stopwords list\n",
    "    stemming -- whether or not to perform stemming\n",
    "    stemmer -- stemming function to use, default is the PorterStemmer from NLTK\n",
    "    \n",
    "    Returns:\n",
    "    cleaned_text -- text with removed stopwords and applied stemming\n",
    "    \n",
    "    '''\n",
    "    #remove stopwords \n",
    "    cleaned_text =  ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        \n",
    "    #perform stemming\n",
    "    if(stemming):\n",
    "        if(tokenizer == 'twitter'):\n",
    "            tokens = TweetTokenizer().tokenize(cleaned_text)\n",
    "            stemmed_tokens = [stemmer.stem(i) for i in tokens]\n",
    "        elif(tokenizer == 'word'):\n",
    "            tokens = word_tokenize(cleaned_text)\n",
    "            stemmed_tokens = [stemmer.stem(i) for i in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        if(tokenizer == 'twitter'):\n",
    "            tokens = TweetTokenizer().tokenize(cleaned_text)\n",
    "        elif(tokenizer == 'word'):\n",
    "            tokens = word_tokenize(cleaned_text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(quotes.shape[0]):\n",
    "    quotes[i] = preprocess_text(quotes[i], 'twitter')\n",
    "for i in range(responses.shape[0]):\n",
    "    responses[i] = preprocess_text(responses[i], 'twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed(corpus,sizeArg=300,windowArg=7,ngrams=3):\n",
    "    \n",
    "    '''\n",
    "    This function generates word2vec embeddings of our corpus. \n",
    "    \n",
    "    Params:\n",
    "    corpus -- our collection of documents. It should be a list of arrays  \n",
    "    sizeArg -- dimension of embedding. Default is 300. This is on the larger side of the spectrum because\n",
    "            our corpus is fairly large (standard embedding size is between 100 and 300).\n",
    "    windowArg -- context window used in embedding. Default is 7, which is a little large. We do this \n",
    "                because sarcasm has a very complex structure and consequently prediction of a token\n",
    "                will require knowledge of a lot of the nearby tokens\n",
    "    ngram -- transforms tokens into phrases of at most n words. Default is three since not many phrases are longer\n",
    "                than three words\n",
    "    \n",
    "    Returns: embedding_matrix -- an array of 300 dimensional vectors, one for each token in the vocabulary of the corpus\n",
    "    '''\n",
    "    \n",
    "    #checks if we need to use the bigram transformer at all\n",
    "    if ngrams==1:\n",
    "        model=Word2Vec(corpus,size=sizeArg,window=windowArg,min_count=5)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        #perform bigramming n times. Note we only perform it a max of 5 times since there are a negligent amount\n",
    "        #of phrases of length bigger than 5\n",
    "        for i in range(0,min(ngrams,5)):\n",
    "            \n",
    "            #Phrases creates an object with all the bigrams, then Phraser is a wrapper class used to access the\n",
    "            #resulting corpus of bigrams. Using phraser also speeds up computation time when making the model\n",
    "            if i==0:\n",
    "                bigram = Phrases(corpus)\n",
    "                bigram_phraser = Phraser(bigram)\n",
    "            else:\n",
    "                bigram = Phrases(bigram_phraser[corpus])\n",
    "                bigram_phraser = Phraser(bigram)\n",
    "        \n",
    "        model=Word2Vec(bigram_phraser[corpus],size=sizeArg,window=windowArg,min_count=5)\n",
    "    \n",
    "    #convert the wv word vectors into a numpy matrix\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), sizeArg))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_features(corpus,valence,emotion):\n",
    "    \n",
    "    '''\n",
    "    This function will create all our lexical vectors.\n",
    "    \n",
    "    Params:\n",
    "    corpus -- our collection of documents. It should be a list of arrays \n",
    "    valence -- a matrix with 2 columns; words in one column and their corresponding AFINN rating in the other. \n",
    "                Note every element in this 2d numpy array is a string.\n",
    "    emotion -- a matrix with 3 columns; emotion in the first column, a word associated with it in the second,\n",
    "                and its correlation in the third. Note every element in this 2d numpy array is a string.\n",
    "                \n",
    "    Returns: lexiconMat, emotions -- an array of lexical feature vectors for each observation and the emotion\n",
    "                labels for each column\n",
    "     -- \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #here we get all the unique emotions in the NRC hashtag database\n",
    "    emotions=np.unique(emotion[:,0])\n",
    "    \n",
    "    #this matrix will hold lexical features for each document. The first len(emotions) features represent \n",
    "    #the precense of a certain emotion in the document. 'Presence' in the document is measured using the \n",
    "    #sum of each word's correlation with a given emotion. The next feature holds the sum of the valence \n",
    "    #values of all the words in the document\n",
    "    lexiconMat=np.zeros((len(corpus),len(emotions)+1))\n",
    "    \n",
    "    #loop through every document and then each word within each document. I tried to figure out a clever way \n",
    "    #to vectorize this but I couldn't\n",
    "    for i in range(0,len(corpus)):\n",
    "        for j in range(0,len(corpus[i])):\n",
    "            \n",
    "            #check if our current word is in the AFINN database. Also make our token lowercase since all the \n",
    "            #words in the AFINN database are lowercase. Capitalization of a word shouldn't change its valence\n",
    "            if str.lower(corpus[i][j]) in valence[:,0]:\n",
    "                \n",
    "                #find which entry in the AFINN database our current word is\n",
    "                indexVal=str.lower(corpus[i][j])==valence[:,0]\n",
    "                \n",
    "                #add this to our valence feature is the lexical matrix. Note valence[indexVal,1] returns an\n",
    "                #array with one element, thats why we added the [0] to the end of it\n",
    "                lexiconMat[i,len(emotions)]+=float(valence[indexVal,1][0])\n",
    "                \n",
    "            #check if our current word is in the AFINN database. Also make our token lowercase since all the \n",
    "            #words in the hashtag-emotion database are lowercase. Capitalization of a word shouldn't change \n",
    "            #the emotion it elicits\n",
    "            if str.lower(corpus[i][j]) in emotion[:,1]:\n",
    "                \n",
    "                #find which entry in the hashtag-emotion database our current word is\n",
    "                indexEmot=str.lower(corpus[i][j])==emotion[:,1]\n",
    "                \n",
    "                #check if our word correlates to more than one emotion \n",
    "                if len(emotion[indexEmot,0])==1:\n",
    "                    \n",
    "                    #generate a truth vector that reports true when on the index of our word's emotion. We append\n",
    "                    #a false value to the end of this because we will pass it in the columns argument when subsetting\n",
    "                    #lexiconMat\n",
    "                    emotionName=emotions==emotion[indexEmot,0]\n",
    "                    emotionName=np.append(emotionName,values=False)\n",
    "                    \n",
    "                    #add the correlation value of our word to corresponding emotion feature. Note we throw a [0]\n",
    "                    #on the end of emotion[indexEmot,2] because that bit of code returns an array with one element\n",
    "                    lexiconMat[i,emotionName]+=float(emotion[indexEmot,2][0])\n",
    "                else:\n",
    "                    \n",
    "                    #loop through all the different emotions our current word correlates to\n",
    "                    for l in range(0,len(emotion[indexEmot,0])):\n",
    "                        \n",
    "                        #do the same stuff as when our word only correlates to one emotion\n",
    "                        emotionName=emotions==emotion[indexEmot,0][l]\n",
    "                        emotionName=np.append(emotionName,values=False)\n",
    "                        lexiconMat[i,emotionName]+=float(emotion[indexEmot,2][l][0])\n",
    "                \n",
    "    #we return emotions here as well so that we have a key for which column in lexiconMat corresponds to which emotion\n",
    "    return (lexiconMat,emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings=embed(quotes)\n",
    "lexFeatures=lexical_features(quotes,afinnMat,emotMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(r'C:\\Users\\Jonathan\\Desktop\\Data Science 2\\word2vec.npy',embeddings)\n",
    "np.save(r'C:\\Users\\Jonathan\\Desktop\\Data Science 2\\lexicalFeatures.npy',lexFeatures[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
