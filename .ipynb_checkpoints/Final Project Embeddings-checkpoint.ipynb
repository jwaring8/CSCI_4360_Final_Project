{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan Waring\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Quote Text  \\\n",
      "0  Michael Phelps Apologizes For \"Regrettable\" Be...   \n",
      "1  Michael Phelps Apologizes For \"Regrettable\" Be...   \n",
      "2  Utah wants to create a database to track the i...   \n",
      "3  Utah wants to create a database to track the i...   \n",
      "4        The Six Million Dead Jews of World War ONE!   \n",
      "\n",
      "                                       Response Text   Subreddit  Label  \n",
      "0  Wow...he smoked pot...oh lord hes such a horri...        news      1  \n",
      "1  Wow, his girlfriend is uhm... Ah fuck it, he's...        news      0  \n",
      "2  I think the government should track every morm...  reddit.com      0  \n",
      "3  Another idea from the party that wants to get ...  reddit.com      1  \n",
      "4  Oh right, *both* wars were just jewish conspir...    politics      1  \n"
     ]
    }
   ],
   "source": [
    "#get our filepaths\n",
    "trainDataPath = 'data/SARC/SARC-train.csv'\n",
    "testDataPath = 'data/SARC/SARC-test.csv'\n",
    "\n",
    "#read in train data\n",
    "trainDataset = pd.read_csv(trainDataPath)\n",
    "trainDataset = trainDataset.dropna(axis=0)\n",
    "trainDataset['Response Text'] = trainDataset['Response Text'].astype(str)\n",
    "print(trainDataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    128540\n",
      "1    128539\n",
      "Name: Label, dtype: int64\n",
      "4902\n"
     ]
    }
   ],
   "source": [
    "print(trainDataset['Label'].value_counts())\n",
    "print(trainDataset['Subreddit'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = trainDataset['Label'].values\n",
    "trainQuotes = trainDataset['Quote Text'].values\n",
    "trainResponses = trainDataset['Response Text'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Quote Text  \\\n",
      "0  \"...two-income families often have even less i...   \n",
      "1  \"...two-income families often have even less i...   \n",
      "2                           Heath Ledger Wins Oscar!   \n",
      "3                           Heath Ledger Wins Oscar!   \n",
      "4                                      atheist toast   \n",
      "\n",
      "                                       Response Text   Subreddit  Label  \n",
      "0  Chalk it up to the ever-increasing cost of fre...    business      1  \n",
      "1  We're about to finally get affordable housing,...    business      0  \n",
      "2   oh wow I am so surprised I never saw this coming  reddit.com      1  \n",
      "3  It'll look so good IN HIS COFFIN NEXT TO HIS C...  reddit.com      0  \n",
      "4    Everything I need to know I learned from toast.     atheism      0  \n"
     ]
    }
   ],
   "source": [
    "# Read in test data\n",
    "testDataset = pd.read_csv(testDataPath)\n",
    "testDataset = testDataset.dropna(axis=0)\n",
    "testDataset['Response Text'] = testDataset['Response Text'].astype(str)\n",
    "print(testDataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32332\n",
      "1    32331\n",
      "Name: Label, dtype: int64\n",
      "2666\n"
     ]
    }
   ],
   "source": [
    "print(testDataset['Label'].value_counts())\n",
    "print(testDataset['Subreddit'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = testDataset['Label'].values\n",
    "testQuotes = testDataset['Quote Text'].values\n",
    "testResponses = testDataset['Response Text'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, tokenizer, stopwords=stopwords.words(\"english\"), stemming=False, stemmer=PorterStemmer()):\n",
    "    '''\n",
    "    This function will remove stopwords from the text and perform stemming. Return tokenized sentences. \n",
    "    \n",
    "    Params:\n",
    "    text -- string we are looking at \n",
    "    tokenizer -- string of either 'twitter' or 'word' to specify which tokenizer to use\n",
    "    stopwords -- list of stopwords to remove, default is the NLTK stopwords list\n",
    "    stemming -- whether or not to perform stemming\n",
    "    stemmer -- stemming function to use, default is the PorterStemmer from NLTK\n",
    "    \n",
    "    Returns:\n",
    "    cleaned_text -- text with removed stopwords and applied stemming\n",
    "    \n",
    "    '''\n",
    "    #remove stopwords \n",
    "    cleaned_text =  ' '.join([word for word in text.split() if word not in stopwords])\n",
    "        \n",
    "    #perform stemming\n",
    "    if(stemming):\n",
    "        if(tokenizer == 'twitter'):\n",
    "            tokens = TweetTokenizer().tokenize(cleaned_text)\n",
    "            stemmed_tokens = [stemmer.stem(i) for i in tokens]\n",
    "        elif(tokenizer == 'word'):\n",
    "            tokens = word_tokenize(cleaned_text)\n",
    "            stemmed_tokens = [stemmer.stem(i) for i in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        if(tokenizer == 'twitter'):\n",
    "            tokens = TweetTokenizer().tokenize(cleaned_text)\n",
    "        elif(tokenizer == 'word'):\n",
    "            tokens = word_tokenize(cleaned_text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run our preprocess text function\n",
    "for i in range(trainQuotes.shape[0]):\n",
    "    trainQuotes[i] = preprocess_text(trainQuotes[i], 'twitter')\n",
    "for i in range(trainResponses.shape[0]):\n",
    "    trainResponses[i] = preprocess_text(trainResponses[i], 'twitter')\n",
    "for i in range(testQuotes.shape[0]):\n",
    "    testQuotes[i] = preprocess_text(testQuotes[i], 'twitter')\n",
    "for i in range(testResponses.shape[0]):\n",
    "    testResponses[i] = preprocess_text(testResponses[i], 'twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(sentences,sizeArg=100,windowArg=7,ngrams=1):\n",
    "    \n",
    "    '''\n",
    "    This function generates word2vec embeddings of our sentences. \n",
    "    \n",
    "    Params:\n",
    "    sentences -- list of tokenized texts (i.e. list of lists of tokens)  \n",
    "    sizeArg -- dimension of embedding. Default is 100. .\n",
    "    windowArg -- context window used in embedding. Default is 7, which is a little large. We do this \n",
    "                because sarcasm has a very complex structure and consequently prediction of a token\n",
    "                will require knowledge of a lot of the nearby tokens\n",
    "    ngram -- transforms tokens into phrases of at most n words\n",
    "    \n",
    "    Returns: \n",
    "    model --  a dictionary mapping word -> sizeArg-dimensional vector\n",
    "    '''\n",
    "    \n",
    "    #checks if we need to use the bigram transformer at all\n",
    "    if ngrams == 1:\n",
    "        model = Word2Vec(size=sizeArg , window=windowArg, min_count = 2, workers=-1, iter=10)\n",
    "        model.build_vocab(sentences)\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "\n",
    "    else:\n",
    "        #perform bigramming n times. Note we only perform it a max of 5 times since there are a negligent amount\n",
    "        #of phrases of length bigger than 5\n",
    "        for i in range(0,min(ngrams,5)):\n",
    "            #Phrases creates an object with all the bigrams, then Phraser is a wrapper class used to access the\n",
    "            #resulting corpus of bigrams. Using phraser also speeds up computation time when making the model\n",
    "            if i==0:\n",
    "                bigram = Phrases(sentences)\n",
    "                bigram_phraser = Phraser(bigram)\n",
    "            else:\n",
    "                bigram = Phrases(bigram_phraser[sentences])\n",
    "                bigram_phraser = Phraser(bigram)\n",
    "        \n",
    "        model = Word2Vec(size=sizeArg, window=windowArg, min_count = 2, workers=-1, iter=10)\n",
    "        model.build_vocab(bigram_phraser[sentences])\n",
    "        model.train(bigram_phraser[sentences], total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVectors(sentences, model, size=100):\n",
    "    '''\n",
    "    This function generates feature vectors from our sentences using word2vec embeddings.  \n",
    "    \n",
    "    Params:\n",
    "    sentences -- list of tokenized texts (i.e. list of lists of tokens)   \n",
    "    model --  a dictionary mapping word -> sizeArg-dimensional vector\n",
    "    size -- should be same as sizeArg from w2v function\n",
    "    \n",
    "    Returns:\n",
    "    feature_matrix -- matrix containing our feature vectors \n",
    "    '''\n",
    "    feature_matrix = np.zeros((sentences.shape[0], size))\n",
    "    for i in range(sentences.shape[0]):        \n",
    "        feature_vector = np.zeros(size)\n",
    "        num_ignored = 0\n",
    "        for j in range(len(sentences[i])):\n",
    "            try:\n",
    "                feature_vector +=  model.wv[sentences[i][j]]\n",
    "            except KeyError:\n",
    "                num_ignored += 1\n",
    "                \n",
    "        feature_vector /= (len(sentences[i]) - num_ignored)\n",
    "        feature_matrix[i] = feature_vector\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainQuote_model = w2v(trainQuotes)\n",
    "train_w2v_Xquote = getFeatureVectors(trainQuotes, trainQuote_model)\n",
    "trainResponse_model = w2v(trainResponses)\n",
    "train_w2v_Xresponse = getFeatureVectors(trainResponses, trainResponse_model)\n",
    "testQuote_model = w2v(testQuotes)\n",
    "test_w2v_Xquote = getFeatureVectors(testQuotes, testQuote_model)\n",
    "testResponse_model = w2v(testResponses)\n",
    "test_w2v_Xresponse = getFeatureVectors(testResponses, testResponse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/numpy/trainQuotes_w2v.npy', train_w2v_Xquote)\n",
    "np.save('data/numpy/trainResponses_w2v.npy', train_w2v_Xresponse)\n",
    "np.save('data/numpy/testQuotes_w2v.npy', test_w2v_Xquote)\n",
    "np.save('data/numpy/testResponses_w2v.npy', test_w2v_Xresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trainQuotes.shape)\n",
    "print(testQuotes.shape)\n",
    "print(train_w2v_Xquote.shape)\n",
    "print(test_w2v_Xquote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NRCdata = {}\n",
    "with open(\"data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\", \"r\", encoding=\"utf-8\") as nrc_file:\n",
    "            for line in nrc_file.readlines():\n",
    "                splited = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                word, emotion, value = splited[0], splited[1], splited[2]\n",
    "                if word in NRCdata.keys():\n",
    "                    NRCdata[word].append((emotion, int(value)))\n",
    "                else:\n",
    "                    NRCdata[word] = [(emotion, int(value))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NRCVectorization(sentences):\n",
    "    '''\n",
    "    This will create NRC lexicon feature vectors. \n",
    "    \n",
    "    Params:\n",
    "    sentences -- list of tokenized texts (i.e. list of lists of tokens)  \n",
    "    \n",
    "    Returns: \n",
    "    wordEmotionMatrix -- feature matrix containing NRC lexicons \n",
    "    '''\n",
    "    wordEmotionMatrix = np.zeros((sentences.shape[0], 10))\n",
    "    for i in range(sentences.shape[0]):\n",
    "        wordEmotionVectors = []\n",
    "        for j in range(len(sentences[i])):\n",
    "            wordEmotVec = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            if sentences[i][j].lower() in NRCdata.keys():\n",
    "                for k in range(len(NRCdata[sentences[i][j].lower()])):\n",
    "                    wordEmotVec[k] = NRCdata[sentences[i][j].lower()][k][1]\n",
    "            wordEmotionVectors.append(wordEmotVec)\n",
    "        wordEmotionVectors = [sum(i) for i in zip(*wordEmotionVectors)]\n",
    "        if wordEmotionVectors != []:\n",
    "            wordEmotionMatrix[i] = wordEmotionVectors\n",
    "        else:\n",
    "            wordEmotionMatrix[i] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    return wordEmotionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLexiconFeatures(sentences):\n",
    "    '''\n",
    "    This will create lexicon feature matrix. \n",
    "    \n",
    "    Params:\n",
    "    sentences -- list of tokenized texts (i.e. list of lists of tokens)  \n",
    "    \n",
    "    Returns: \n",
    "    lexiconMatrix -- feature matrix containing lexicons\n",
    "    '''\n",
    "    \n",
    "    emotionMatrix = NRCVectorization(sentences)\n",
    "    afinnVector = np.zeros((sentences.shape[0], 1))\n",
    "    afinn = Afinn()\n",
    "    for i in range(sentences.shape[0]):\n",
    "        afinnVector[i] = afinn.score(\" \".join(sentences[i]))\n",
    "    lexiconMatrix = np.hstack((emotionMatrix, afinnVector))\n",
    "    return lexiconMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQuotes_lex = getLexiconFeatures(trainQuotes)\n",
    "trainResponses_lex = getLexiconFeatures(trainResponses)\n",
    "testQuotes_lex = getLexiconFeatures(testQuotes)\n",
    "testResponses_lex = getLexiconFeatures(testResponses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/numpy/trainQuotes_lex.npy', trainQuotes_lex)\n",
    "np.save('data/numpy/trainResponses_lex.npy', trainResponses_lex)\n",
    "np.save('data/numpy/testQuotes_lex.npy', testQuotes_lex)\n",
    "np.save('data/numpy/testResponses_lex.npy', testResponses_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
